{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.google.com/document/d/118KdHMFEdTX6ghUDTG2Ev_qLDcZTM5HxKsT4JG6gLcA/edit\n",
    "Main Task\n",
    "\n",
    "You have to use the above dataset to build your recommender system.\n",
    "Use of any other extra data is prohibited.\n",
    "You can use pretrained models though and do some transfer learning.\n",
    "Use NLP to build the recommender system. More points on how you use your NLP skills to build the recommender system.\n",
    "You’ll have to use the overview of the movie and title compulsorily as they are related to your NLP transformations.\n",
    "You can use other data given in the dataset as well, totally depends on you what you have to use and what not.\n",
    "The output should give recommendations based on 1. Movie title 2. Movie overview 3. Movie genre.\n",
    "You can think of more recommendations all by yourself. For eg. similar movies by year.\n",
    "\n",
    "The optional bonus tasks you can do after you’ve completed the original task:\n",
    "\n",
    "Bonus Task 1\n",
    "\n",
    "Do topic modelling based on the text data available for movies.\n",
    "\n",
    "Bonus Task 2\n",
    "\n",
    "Deploy your models to get recommendation over HTTP APIs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting contextual features using Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Dependencies\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import tensorflow as tf\n",
    "import bert\n",
    "from bert.extract_features import *\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=tf.Session()\n",
    "\n",
    "loss= -(y*log(ypred)+((1-y)*log(1-ypred)))\n",
    "\n",
    "tfidf=tf * log(N/those doc having the word)\n",
    "\n",
    "king-man+woman = queen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45466, 3)\n",
      "(44512, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#simplifying data : extracting -> title , overview and genres\n",
    "\n",
    "data=pd.read_csv(\"./movies_metadata.csv\")\n",
    "\n",
    "def make_dataset(data):\n",
    "    data['tagline']=data[\"tagline\"].fillna(\" \")\n",
    "    data=data.loc[:,[\"original_title\",\"genres\",\"overview\"]]\n",
    "    print(data.shape)\n",
    "    data=data.dropna()  #dropping movies with no overview or genres\n",
    "    print(data.shape)\n",
    "#     data['description']=data.loc[:,\"overview\"]+data.loc[:,\"tagline\"]\n",
    "    return data.loc[:,[\"original_title\",\"genres\"]],data.loc[:,\"overview\"]\n",
    "\n",
    "data_info,data_view=make_dataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Max len comes out to be 187 hence taking it 256 according to bert.\n",
    "\n",
    "# y=data_view.apply(lambda x : x.split(\" \"))\n",
    "# max_len=0\n",
    "# for i in range(y.shape[0]):\n",
    "#     if len(y.iloc[i])> max_len:\n",
    "#         max_len=len(y.iloc[i])\n",
    "# print(max_len)\n",
    "\n",
    "max_len=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Tf hub to get details about the vocabulary etc of bert.\n",
    "\n",
    "bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "\n",
    "max_seq_length = 64\n",
    "\n",
    "def create_tokenizer_from_hub_module(bert_path,sess):     # gives bert tokenizer\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    bert_module =  hub.Module(bert_path)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    vocab_file, do_lower_case = sess.run(\n",
    "        [\n",
    "            tokenization_info[\"vocab_file\"],\n",
    "            tokenization_info[\"do_lower_case\"],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return bert.tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#extract features.py from google research/Bert Github \n",
    "def extract_emb(features,layers=[-1,-2,-3,-4],max_seq_len=64):  #extract features\n",
    "    \n",
    "    layer_indexes = [int(x) for x in layers]              #last 4 hidden layers\n",
    "\n",
    "    bert_config = bert.modeling.BertConfig.from_json_file(\"./tmp/wwm_uncased_L-24_H-1024_A-16/bert_config.json\")#model configs\n",
    "\n",
    "    tokenizer = bert.tokenization.FullTokenizer(\n",
    "      vocab_file=\"./tmp/wwm_uncased_L-24_H-1024_A-16/vocab.txt\", do_lower_case=True)\n",
    "\n",
    "    is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2    #\n",
    "    run_config = tf.contrib.tpu.RunConfig(                          #\n",
    "      master=None,                                                  #    Setting up configurations\n",
    "      tpu_config=tf.contrib.tpu.TPUConfig(                          #  \n",
    "          num_shards=None,                                          #\n",
    "          per_host_input_for_training=is_per_host))                 #   \n",
    "\n",
    "    unique_id_to_feature = {}\n",
    "    for feature in features:\n",
    "        unique_id_to_feature[feature.unique_id] = feature\n",
    "\n",
    "    model_fn = model_fn_builder(                                        #\n",
    "      bert_config=bert_config,                                          #\n",
    "      init_checkpoint=\"./tmp/wwm_uncased_L-24_H-1024_A-16/bert_model.ckpt\",  #  funtion to build pretrained model\n",
    "      layer_indexes=layer_indexes,                                      #\n",
    "      use_tpu=False,                                                    # \n",
    "      use_one_hot_embeddings=False)                                     #\n",
    "  \n",
    "    estimator = tf.contrib.tpu.TPUEstimator(                            #\n",
    "      use_tpu=False,                                                    #\n",
    "      model_fn=model_fn,                                                # predictor to extract features \n",
    "      config=run_config,                                                #\n",
    "      predict_batch_size=12)                                            #\n",
    "\n",
    "    input_fn = input_fn_builder(features=features, seq_length=max_seq_len)   #prepares input\n",
    "    \n",
    "    trained_emb_data={} \n",
    "    i=0\n",
    "    for result in estimator.predict(input_fn, yield_single_examples=True):\n",
    "        l1=np.mean(axis=0,a=result['layer_output_0'])     #\n",
    "        l2=np.mean(axis=0,a=result['layer_output_1'])     # Last 4 hidden layers features\n",
    "        l3=np.mean(axis=0,a=result['layer_output_2'])     # \n",
    "        l4=np.mean(axis=0,a=result['layer_output_3'])     #\n",
    "\n",
    "        emb=np.mean(axis=0,a=[l1,l2,l3,l4])               # taking mean to get sentence embeddings\n",
    "        trained_emb_data[data_info.iloc[i,0]]=emb         # saving with movie titles\n",
    "        i+=1\n",
    "    return trained_emb_data                               # returning the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
    "train_InputExamples = data_view.apply(lambda x: InputExample(unique_id=0, # Globally unique ID for bookkeeping, unused in this example\n",
    "                                                                   text_a = x, \n",
    "                                                                   text_b = None))\n",
    "# Instantiate tokenizer\n",
    "tokenizer = create_tokenizer_from_hub_module(bert_path,sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=convert_examples_to_features(train_InputExamples\n",
    "                                        ,max_len,tokenizer,) #converting to features expected by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0903 18:47:57.015343 13340 estimator.py:1984] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x000001CE420D1D90>) includes params argument, but params are not passed to Estimator.\n",
      "W0903 18:47:57.019330 13340 estimator.py:1811] Using temporary folder as model directory: C:\\Users\\Dell\\AppData\\Local\\Temp\\tmpkj539vy6\n",
      "W0903 18:47:57.021613 13340 tpu_context.py:211] eval_on_tpu ignored because use_tpu is False.\n"
     ]
    }
   ],
   "source": [
    "# emb=extract_emb(train_data,max_seq_len=max_len) # extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./emb.pickle\",\"wb\") as file:   #saving embeddings\n",
    "#     pkl.dump(emb,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests,bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "html=requests.get(\"https://www.analyticsvidhya.com/blog/2017/05/41-questions-on-statisitics-data-scientists-analysts/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "html=bs4.BeautifulSoup(html.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<strong>1) Which of these measures are used to analyze the central tendency of data? </strong>,\n",
       " <strong>Solution: (B)</strong>,\n",
       " <strong>Solution: (D)</strong>,\n",
       " <strong>Solution: (A)</strong>,\n",
       " <strong>Solution: (A)</strong>,\n",
       " <strong>5) Below, we have represented six data points on a scale where vertical lines on scale represent unit. </strong>,\n",
       " <strong>Which of the following line represents the mean of the given data points, where the scale is divided into same units?</strong>,\n",
       " <strong>Solution: (C)</strong>,\n",
       " <strong>6) If a positively skewed distribution has a median of 50, which of the following statement is true? </strong>,\n",
       " <strong>Solution: (E)</strong>,\n",
       " <strong>7) Which of the following is a possible value for the median of the below distribution?</strong>,\n",
       " <strong>Solution: (B)</strong>,\n",
       " <strong>8) Which of the following statements are true about Bessels Correction while calculating a sample standard deviation?</strong>,\n",
       " <strong>Bessels correction is always done when we perform any operation on a sample data.</strong>,\n",
       " <strong>Bessels correction is used when we are trying to estimate population standard deviation from the sample.</strong>,\n",
       " <strong>Bessels corrected standard deviation is less biased.</strong>,\n",
       " <strong>Solution: (C)</strong>,\n",
       " <strong>Solution: (A)</strong>,\n",
       " <strong>Solution: (B)</strong>,\n",
       " <strong>11) Standard deviation is robust to outliers?</strong>,\n",
       " <strong>Solution: (B)</strong>,\n",
       " <strong>12) For the below normal distribution, which of the following option holds true ?</strong>,\n",
       " <strong>σ1, σ2 and σ3 represent the standard deviations for curves 1, 2 and 3 respectively.</strong>,\n",
       " <strong>Solution: (B)</strong>,\n",
       " <strong>Solution: (A)</strong>,\n",
       " <strong>Solution: (A)</strong>,\n",
       " <strong>Context for Questions 15-17</strong>,\n",
       " <strong>Studies show that listening to music while studying can improve your memory. To demonstrate this, a researcher obtains a sample of 36 college students and gives them a standard memory test while they listen to some background music. Under normal circumstances (without music), the mean score obtained was 25 and standard deviation is 6. The mean score for the sample after the experiment (i.e With music) is 28.</strong>,\n",
       " <strong>Solution: (D)</strong>,\n",
       " <strong>Solution: (B)</strong>,\n",
       " <strong>Solution: (B)</strong>,\n",
       " <strong>Solution: (D)</strong>,\n",
       " <strong>Solution: (B)</strong>,\n",
       " <strong>Context for questions 20- 22</strong>,\n",
       " <strong>A medical doctor wants to reduce blood sugar level of all his patients by altering their diet. He finds that the mean sugar level of all patients is 180 with a standard deviation of 18. Nine of his patients start dieting and the mean of the sample is observed to 175. Now, he is considering to recommend all his patients to go on a diet.</strong>,\n",
       " <strong>Note: He calculates 99% confidence interval.</strong>,\n",
       " <strong>Solution: (B)</strong>,\n",
       " <strong>Solution: (A)</strong>,\n",
       " <strong>Solution: (B) </strong>,\n",
       " <strong>Question Context 23-25</strong>,\n",
       " <strong>A researcher is trying to examine the effects of two different teaching methods. He divides 20 students into two groups of 10 each. For group 1, the teaching method is using fun examples. Where as for group 2 the teaching method is using software to help students learn. After a 20 minutes lecture of both groups, a test is conducted for all the students.</strong>,\n",
       " <strong>We want to calculate if there is a significant difference in the scores of both the groups.</strong>,\n",
       " <strong>It is given that:</strong>,\n",
       " <strong>23) What is the value of t-statistic?</strong>,\n",
       " <strong>Solution: (A)</strong>,\n",
       " <strong>24) Is there a significant difference in the scores of the two groups?</strong>,\n",
       " <strong>Solution: (A)</strong>,\n",
       " <strong>Solution: (A)</strong>,\n",
       " <strong>Solution: (A)</strong>,\n",
       " <strong>Solution: (B)</strong>,\n",
       " <strong>Solution: (C)</strong>,\n",
       " <strong>High correlation implies that after exercise the test scores are high.</strong>,\n",
       " <strong>Correlation does not imply causation.</strong>,\n",
       " <strong>Correlation measures the strength of linear relationship between amount of exercise and test scores.</strong>,\n",
       " <strong>Solution: (C)</strong>,\n",
       " <strong>Solution: (B)</strong>,\n",
       " <strong>Solution: (B)</strong>,\n",
       " <strong>Solution: (C)</strong>,\n",
       " <strong>Solution: (A)</strong>,\n",
       " <strong>Solution: (D)</strong>,\n",
       " <strong>Solution: (B)</strong>,\n",
       " <strong>Solution: (B)</strong>,\n",
       " <strong>Solution: (B)</strong>,\n",
       " <strong>Solution: (D)</strong>,\n",
       " <strong>Which of the following is a MAE (Mean Absolute Error) for this linear model?</strong>,\n",
       " <strong>Solution: (A)</strong>,\n",
       " <strong>Solution:  (B)</strong>,\n",
       " <strong>Solution: (A)</strong>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html.find_all(\"strong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
